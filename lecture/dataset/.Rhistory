knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
require(ISLR)
require(boot)
?cv.glm
plot(mpg~horsepower,data=Auto)
glm.fit=glm(mpg~horsepower, data=Auto)
summary(glm.fit)
cv.glm(Auto,glm.fit)$delta #pretty slow (doesnt use formula (5.2) on page 180)
loocv=function(fit){
h=lm.influence(fit)$h
# Element by element devidion
mean((residuals(fit)/(1-h))^2)
}
loocv(glm.fit)
cv.error=rep(0,5)
cv.error=rep(0,5)
cv.error=rep(0,5)
degree=1:5
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error[d]=loocv(glm.fit)
}
plot(degree,cv.error,type="b")
glm.fit=glm(mpg~horsepower, data=Auto)
summary(glm.fit)
?cv.glm
summary(glm.fit)
?glm
summary(glm.fit)
?glm.fit
summary(glm.fit)
?cv.glm
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
alpha(Portfolio$X,Portfolio$Y)
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Portfolio,1:100)
summary(Portfolio)
str(Portfolio)
set.seed(1)
alpha.fn (Portfolio,sample(1:100,100,replace=TRUE))
boot.out=boot(Portfolio,alpha.fn,R=1000)
boot.out
plot(boot.out)
library(JGR)
JGR()
data <- 10 * c(11)
data <- 10 * c(11)
print(data)
age11 <- rep(11,10)
age12 <- rep(12,9)
age13 <- rep(13,11)
age14 <- rep(14,14)
age15 <- rep(15,10)
age16 <- rep(16,6)
data <- c(age11,age12,age13,age14,age15,age16)
mean(data)
sd(data)
median(data)
IQR(data)
hist(data)
hist(data,binwidth =1)
?hist
hist(data,breaks= c(11,12,13,14,15,16))
hist(data,breaks= c(11,12,13,14,15,16,17))
hist(data,breaks= c(10,11,12,13,14,15,16,17))
hist(data,breaks= c(10,11,12,13,14,15,16))
library(ggplot2)
qplot(data)
question2Data <- c(rep(0,53),rep(1,47))
mean(question2Data)
sd(question2Data)
median(question2Data)
IQR(question2Data)
data <- c(age11,age12,age13,age14,age15,age16)
mean(data)
sd(data)
median(data)
print(data)
IQR(data)
question2Data <- c(rep(0,53),rep(1,47))
mean(question2Data)
sd(question2Data)
median(question2Data)
IQR(question2Data)
library(JGR)
JGR()
a.w=2
a.c =3
a
(0.29)^2
(0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.2
(0-0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.2
(0-0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.02
sqrt((0-0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.02)
0.5^5 * 5 * 100
(81-75)/sqrt(150*0.5^2)
sqrt(500*0.05*0.95)
library(JGR)
JGR()
aug2 <- read.csv("C:/Users/zhushun0008/Desktop/aug2.csv", header=F)
View(aug2)
install.packages("igraph")
require(igraph)
a<-read.graph('METHODSC.NET', 'pajek')
METHODSC <- read.table("F:/SkyDrive/Studying/Stanford/SocialAndEconomicNetworksModelsAnalysis/homework/PA01/METHODSC.NET", quote="\"")
View(METHODSC)
framingham = read.csv("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset/framingham.csv")
str(framingham)
library(caTools)
train = subset(framingham, split==TRUE)
test = subset(framingham, split==FALSE)
# Logistic Regression Model
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
sets typically between 50% and 80% in the training set
set.seed(1000)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
# Split up the data using subset
train = subset(framingham, split==TRUE)
test = subset(framingham, split==FALSE)
# Logistic Regression Model
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
predictTest = predict(framinghamLog, type="response", newdata=test)
table(test$TenYearCHD, predictTest > 0.5)
(1069+11)/(1069+6+187+11)
(1069+6)/(1069+6+187+11)
# Test set AUC
library(ROCR)
ROCRpred = prediction(predictTest, test$TenYearCHD)
as.numeric(performance(ROCRpred, "auc")@y.values)
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
PredictROC
PredictCART = predict(StevensTree, newdata = Test, type = "class")
table(Test$Reverse, PredictCART)
(41+71)/(41+36+22+71)
# ROC curve
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(minbucket=25))
# plot our tree using the prp function
# CRI is short for Criminal Defendant, INJ is short for Injured Person, etc...
prp(StevensTree)
# Make predictions
PredictCART = predict(StevensTree, newdata = Test, type = "class")
table(Test$Reverse, PredictCART)
(41+71)/(41+36+22+71)
# ROC curve
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(minbucket=25))
library(rpart)
# install.packages("rpart.plot")
stevens = read.csv("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset/stevens.csv")
# 1. Docket: just a unique identifier for each case
# 2. Term: the year of the case
# 3. six independent variables:
#       the circuit court of origin
#	the issue area of the case
#	the type of petitioner
#	the type of respondent
#	the lower court direction
#	whether or not the petitioner argued that a law or practice was unconstitutional
# 4. whether or not Justice Stevens voted to reverse the case: 1 for reverse and 0 for affirm
str(stevens)
# Split the data
library(caTools)
set.seed(3000)
split = sample.split(stevens$Reverse, SplitRatio = 0.7)
Train = subset(stevens, split==TRUE)
Test = subset(stevens, split==FALSE)
# Install rpart library that contains CART
# install.packages("rpart")
library(rpart)
# install.packages("rpart.plot")
library(rpart.plot)
# CART model
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(minbucket=25))
# plot our tree using the prp function
# CRI is short for Criminal Defendant, INJ is short for Injured Person, etc...
prp(StevensTree)
# Make predictions
PredictCART = predict(StevensTree, newdata = Test, type = "class")
table(Test$Reverse, PredictCART)
(41+71)/(41+36+22+71)
# ROC curve
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
PredictROC
PredictROC = predict(StevensTree, newdata = Test)
PredictROC
pred = prediction(PredictROC[,2], Test$Reverse)
perf = performance(pred, "tpr", "fpr")
plot(perf)
install.packages("randomForest")
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
library(randomForest)
library(randomForest)
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
Train$Reverse = as.factor(Train$Reverse)
Test$Reverse = as.factor(Test$Reverse)
# Try again
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
# Make predictions
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest)
(40+74)/(40+37+19+74)
install.packages("caret")
install.packages("e1071")
library(caret)
library(e1071)
fitControl = trainControl( method = "cv", number = 10 )
# use cp values from 0.01 through 0.5
cartGrid = expand.grid( .cp = (1:50)*0.01)
# Perform the cross validation
# Validate our parameters for our CART tree
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "rpart", trControl = fitControl, tuneGrid = cartGrid )
# Create a new CART model
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(cp = 0.18))
# Make predictions
# Select the optimal model using the largest value
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(cp = 0.18))
# Make predictions
PredictCV = predict(StevensTreeCV, newdata = Test, type = "class")
table(Test$Reverse, PredictCV)
(59+64)/(59+18+29+64)
table(WHO$Region)
---
title: "Code for Week01 In Class"
author: "Shun Zhu"
date: "Saturday, January 10, 2015"
output: pdf_document
---
### VIDEO 02: GETTING STARTED IN R
```{r}
# Basic Calculations
8*6
2^16
# you will see plus sign and wait for you completing the command
# you Coud complete the command or hit Excape
2^
8*6
8*10
# Functions could take several arguments
# Build-in functions
# Install packages
sqrt(2)
abs(-65)
# Get help of any functions
?sqrt
# Variables
# 1. DO not use spaces in variable names
# using a mix of capital and lowercase letters
# 2. Do not start variable names with a number.
# 3. Case Sensitive
SquareRoot2 = sqrt(2)
SquareRoot2
HoursYear <- 365*24
HoursYear
#  List all of the variables that you've created in your current R session
ls()
```
### VIDEO 03: VECTORS AND DATA FRAMES
```{r}
# 01. c() indicates combining same objects in Columns
c(2,3,5,8,13)
# 02
Country = c("Brazil", "China", "India", "Switzerland", "USA")
LifeExpectancy = c(74, 76, 65, 83, 79)
# 03 Automatically Convert numbers into string
c("Brazil", 74, "China", 76)
# 04
Country[1]
LifeExpectancy[3]
# 05
Sequence = seq(1, 100, 2)
# 06 Maintain the type of original object
CountryData = data.frame(Country, LifeExpectancy)
CountryData
# 07 Column Comibine
# R will just combine the vectors in the order they're typed
Population = c(199000, 1390000, 1240000, 1997, 318000)
CountryData = cbind(Data, Population)
CountryData
# 08. Add new observations with row combining
Country = c("Australia", "Greece")
LifeExpectancy = c(81, 82)
Population = c(23050, 11125)
NewCountryData = data.frame(Country, LifeExpectancy, Population)
AllCountryData = rbind(CountryData, NewCountryData)
AllCountryData
```
### VIDEO 04: LOADING DATA FILES
```{r}
# 1. Change working directory that has WHO.csv(World Health Organization)
getwd()
setwd("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset")
# 2. Loading csv files
WHO = read.csv("WHO.csv")
# 3. str function shows the structure of the data
#  the name of the country
# the region the country is in
# the population in thousands
# the percentage of the population under 15
# the percentage of the population over 60
# the fertility rate or average number of children per woman
# the life expectancy in years
# the child mortality rate which is the number of children who die by age five per 1,000 births
# the number of cellular subscribers per 100 population
# the literacy rate among adults aged greater than or equal to 15
# the gross national income per capital
# the percentage of male children enrolled in primary school
# he percentage of female children enrolled in primary school
str(WHO)
# 4. summary function gives a numerical summary of each of our variables
summary(WHO)
# 5. The subset function takes two arguments. The first is the data frame we want to take a subset of, in this case, WHO. And the second argument is the criteria for which observations of WHO should belong in our new data
WHO_Europe = subset(WHO, Region == "Europe")
str(WHO_Europe)
# 6. save this new data frame, WHO_Europe, to a csv file
write.csv(WHO_Europe, "WHO_Europe.csv")
# 7. Removing variables in the working space to save used space
ls()
rm(WHO_Europe)
```
### VIDEO 05: DATA ANALYSIS
```{r}
# access a variable in a data frame
WHO$under15
# Basic data analysis
mean(WHO$Under15)
sd(WHO$Under15)
summary(WHO$Under15)
# There's a country with only 13.12% (minimun) of the population under 15. Let's see which one it is.
which.min(WHO$Under15)
WHO$Country[86]
which.max(WHO$Under15)
WHO$Country[124]
sort(WHO$Under15)
# Scatterplot
plot(WHO$GNI, WHO$FertilityRate)
# Subsetting
Outliers = subset(WHO, GNI > 10000 & FertilityRate > 2.5)
#  The number of rows in the dataset
nrow(Outliers)
# Extract a few variables from a data set
Outliers[c("Country","GNI","FertilityRate")]
```
### VIDEO 06
```{r}
# Histograms
hist(WHO$CellularSubscribers)
# Boxplot sorted by Region
# Outliers are defined by first computing the difference between the first and third quartiles, or the height of the box. This number is called the inter-quartile range. Any point that is greater than the third quartile plus the inter-quartile range, or any point that is less than the first quartile minus the inter-quartile range is considered an outlier
boxplot(WHO$LifeExpectancy ~ WHO$Region)
boxplot(WHO$LifeExpectancy ~ WHO$Region, xlab = "Region", ylab = "Life Expectancy", main = "Life Expectancy of Countries by Region")
# Summary Tables
table(WHO$Region)
tapply(WHO$Over60, WHO$Region, mean)
tapply(WHO$LiteracyRate, WHO$Region, min)
tapply(WHO$LiteracyRate, WHO$Region, min, na.rm=TRUE)
```
### VIDEO 06: SAVING WITH SCRIPT FILES
hist(WHO$CellularSubscribers)
table(WHO$Region)
