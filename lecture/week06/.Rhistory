table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
# Use another k = 3
knn.pred3=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 3)
table(knn.pred3,Direction[!train])
# Here k = 3 reduces performance, 0.5357
mean(knn.pred3==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
knn.pred2=knn(Xlag[train,],Xlag[!train,],Direction[train],k = 2)
table(knn.pred2,Direction[!train])
# Here k = 2  improves performance, 0.5595238
mean(knn.pred2==Direction[!train])
require(ISLR)
require(boot)
?cv.glm
plot(mpg~horsepower,data=Auto)
glm.fit=glm(mpg~horsepower, data=Auto)
summary(glm.fit)
cv.glm(Auto,glm.fit)$delta #pretty slow (doesnt use formula (5.2) on page 180)
loocv=function(fit){
h=lm.influence(fit)$h
# Element by element devidion
mean((residuals(fit)/(1-h))^2)
}
loocv(glm.fit)
cv.error=rep(0,5)
cv.error=rep(0,5)
cv.error=rep(0,5)
degree=1:5
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error[d]=loocv(glm.fit)
}
plot(degree,cv.error,type="b")
glm.fit=glm(mpg~horsepower, data=Auto)
summary(glm.fit)
?cv.glm
summary(glm.fit)
?glm
summary(glm.fit)
?glm.fit
summary(glm.fit)
?cv.glm
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
alpha(Portfolio$X,Portfolio$Y)
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Portfolio,1:100)
summary(Portfolio)
str(Portfolio)
set.seed(1)
alpha.fn (Portfolio,sample(1:100,100,replace=TRUE))
boot.out=boot(Portfolio,alpha.fn,R=1000)
boot.out
plot(boot.out)
library(JGR)
JGR()
data <- 10 * c(11)
data <- 10 * c(11)
print(data)
age11 <- rep(11,10)
age12 <- rep(12,9)
age13 <- rep(13,11)
age14 <- rep(14,14)
age15 <- rep(15,10)
age16 <- rep(16,6)
data <- c(age11,age12,age13,age14,age15,age16)
mean(data)
sd(data)
median(data)
IQR(data)
hist(data)
hist(data,binwidth =1)
?hist
hist(data,breaks= c(11,12,13,14,15,16))
hist(data,breaks= c(11,12,13,14,15,16,17))
hist(data,breaks= c(10,11,12,13,14,15,16,17))
hist(data,breaks= c(10,11,12,13,14,15,16))
library(ggplot2)
qplot(data)
question2Data <- c(rep(0,53),rep(1,47))
mean(question2Data)
sd(question2Data)
median(question2Data)
IQR(question2Data)
data <- c(age11,age12,age13,age14,age15,age16)
mean(data)
sd(data)
median(data)
print(data)
IQR(data)
question2Data <- c(rep(0,53),rep(1,47))
mean(question2Data)
sd(question2Data)
median(question2Data)
IQR(question2Data)
library(JGR)
JGR()
a.w=2
a.c =3
a
(0.29)^2
(0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.2
(0-0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.2
(0-0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.02
sqrt((0-0.29)^2 * 0.8 + (1-0.29)^2 * 0.13 + (2-0.29)^2 * 0.05 + (3-0.29)^2 * 0.02)
0.5^5 * 5 * 100
(81-75)/sqrt(150*0.5^2)
sqrt(500*0.05*0.95)
library(JGR)
JGR()
aug2 <- read.csv("C:/Users/zhushun0008/Desktop/aug2.csv", header=F)
View(aug2)
install.packages("igraph")
require(igraph)
a<-read.graph('METHODSC.NET', 'pajek')
METHODSC <- read.table("F:/SkyDrive/Studying/Stanford/SocialAndEconomicNetworksModelsAnalysis/homework/PA01/METHODSC.NET", quote="\"")
View(METHODSC)
framingham = read.csv("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset/framingham.csv")
str(framingham)
library(caTools)
train = subset(framingham, split==TRUE)
test = subset(framingham, split==FALSE)
# Logistic Regression Model
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
sets typically between 50% and 80% in the training set
set.seed(1000)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
# Split up the data using subset
train = subset(framingham, split==TRUE)
test = subset(framingham, split==FALSE)
# Logistic Regression Model
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
predictTest = predict(framinghamLog, type="response", newdata=test)
table(test$TenYearCHD, predictTest > 0.5)
(1069+11)/(1069+6+187+11)
(1069+6)/(1069+6+187+11)
# Test set AUC
library(ROCR)
ROCRpred = prediction(predictTest, test$TenYearCHD)
as.numeric(performance(ROCRpred, "auc")@y.values)
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
PredictROC
PredictCART = predict(StevensTree, newdata = Test, type = "class")
table(Test$Reverse, PredictCART)
(41+71)/(41+36+22+71)
# ROC curve
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(minbucket=25))
# plot our tree using the prp function
# CRI is short for Criminal Defendant, INJ is short for Injured Person, etc...
prp(StevensTree)
# Make predictions
PredictCART = predict(StevensTree, newdata = Test, type = "class")
table(Test$Reverse, PredictCART)
(41+71)/(41+36+22+71)
# ROC curve
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(minbucket=25))
library(rpart)
# install.packages("rpart.plot")
stevens = read.csv("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset/stevens.csv")
# 1. Docket: just a unique identifier for each case
# 2. Term: the year of the case
# 3. six independent variables:
#       the circuit court of origin
#	the issue area of the case
#	the type of petitioner
#	the type of respondent
#	the lower court direction
#	whether or not the petitioner argued that a law or practice was unconstitutional
# 4. whether or not Justice Stevens voted to reverse the case: 1 for reverse and 0 for affirm
str(stevens)
# Split the data
library(caTools)
set.seed(3000)
split = sample.split(stevens$Reverse, SplitRatio = 0.7)
Train = subset(stevens, split==TRUE)
Test = subset(stevens, split==FALSE)
# Install rpart library that contains CART
# install.packages("rpart")
library(rpart)
# install.packages("rpart.plot")
library(rpart.plot)
# CART model
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(minbucket=25))
# plot our tree using the prp function
# CRI is short for Criminal Defendant, INJ is short for Injured Person, etc...
prp(StevensTree)
# Make predictions
PredictCART = predict(StevensTree, newdata = Test, type = "class")
table(Test$Reverse, PredictCART)
(41+71)/(41+36+22+71)
# ROC curve
library(ROCR)
PredictROC = predict(StevensTree, newdata = Test)
PredictROC
PredictROC = predict(StevensTree, newdata = Test)
PredictROC
pred = prediction(PredictROC[,2], Test$Reverse)
perf = performance(pred, "tpr", "fpr")
plot(perf)
install.packages("randomForest")
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
library(randomForest)
library(randomForest)
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
Train$Reverse = as.factor(Train$Reverse)
Test$Reverse = as.factor(Test$Reverse)
# Try again
StevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )
# Make predictions
PredictForest = predict(StevensForest, newdata = Test)
table(Test$Reverse, PredictForest)
(40+74)/(40+37+19+74)
install.packages("caret")
install.packages("e1071")
library(caret)
library(e1071)
fitControl = trainControl( method = "cv", number = 10 )
# use cp values from 0.01 through 0.5
cartGrid = expand.grid( .cp = (1:50)*0.01)
# Perform the cross validation
# Validate our parameters for our CART tree
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "rpart", trControl = fitControl, tuneGrid = cartGrid )
# Create a new CART model
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(cp = 0.18))
# Make predictions
# Select the optimal model using the largest value
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, method="class", data = Train, control=rpart.control(cp = 0.18))
# Make predictions
PredictCV = predict(StevensTreeCV, newdata = Test, type = "class")
table(Test$Reverse, PredictCV)
(59+64)/(59+18+29+64)
setwd("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/week05")
tweets = read.csv("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset/tweets.csv", stringsAsFactors=FALSE)
str(tweets)
tweets$Negative = as.factor(tweets$Avg <= -1)
table(tweets$Negative)
install.packages("tm")
library(tm)
install.packages("SnowballC")
library(SnowballC)
# Create corpus
corpus = Corpus(VectorSource(tweets$Tweet))
# Look at corpus
corpus
corpus[[1]]
# Each operation, like stemming or removing stop words, can be done with one line in R
corpus = tm_map(corpus, tolower)
corpus[[1]]
corpus = tm_map(corpus, removePunctuation)
corpus[[1]]
stopwords("english")[1:10]
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
corpus[[1]]
corpus = tm_map(corpus, stemDocument)
corpus[[1]]
frequencies = DocumentTermMatrix(corpus)
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
frequencies = tm.DocumentTermMatrix(corpus)
?DocumentTermMatrix
corpus[[1]]
frequencies = DocumentTermMatrix(corpus)
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
corpus[[1]]
# Stem document
corpus = tm_map(corpus, stemDocument)
corpus[[1]]
```
### Video 06
```{r}
# Create matrix
frequencies = DocumentTermMatrix(corpus)
frequencies
library(caTools)
set.seed(123)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
library(tm)
library(SnowballC)
tweets = read.csv("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset/tweets.csv", stringsAsFactors=FALSE)
str(tweets)
# Create dependent variable
tweets$Negative = as.factor(tweets$Avg <= -1)
table(tweets$Negative)
# Install new packages
# install.packages("tm")
library(tm)
#install.packages("SnowballC")
library(SnowballC)
# Create corpus
corpus = Corpus(VectorSource(tweets$Tweet))
# Look at corpus
corpus
corpus[[1]]
# Convert to lower-case
# Each operation, like stemming or removing stop words, can be done with one line in R
corpus = tm_map(corpus, tolower)
corpus[[1]]
# Remove punctuation
corpus = tm_map(corpus, removePunctuation)
corpus[[1]]
# Look at stop words
stopwords("english")[1:10]
# Remove stopwords and apple
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
corpus[[1]]
# Stem document
corpus = tm_map(corpus, stemDocument)
corpus[[1]]
frequencies = DocumentTermMatrix(corpus)
corpus
str(tweets)
frequencies = as.DocumentTermMatrix(corpus)
corpus
corpus[[1]]
frequencies = DocumentTermMatrix(as.list(corpus)
frequencies = DocumentTermMatrix(as.list(corpus))
frequencies = DocumentTermMatrix(as.list(corpus))
frequencies = DocumentTermMatrix(as.data.frame(corpus))
frequencies = DocumentTermMatrix(corpus)
stopwords("english")
library(rpart)
library(rpart.plot)
(294+18)/(294+6+37+18)
300/(300+55)
library(randomForest)
set.seed(123)
(293+21)/(293+7+34+21)
(293+21)/(293+7+34+21)
setwd("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/week06")
movies = read.table("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset/movieLens.txt", header=FALSE, sep="|",quote="\"")
movies = read.table("F:/SkyDrive/Studying/MIT_COURSES/15-071-TheAnalyticsEdge/lecture/dataset/movieLens.txt", header=FALSE, sep="|",quote="\"")
str(movies)
colnames(movies) = c("ID", "Title", "ReleaseDate", "VideoReleaseDate", "IMDB", "Unknown", "Action", "Adventure", "Animation", "Childrens", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western")
str(movies)
# Remove unnecessary variables
movies$ID = NULL
movies$ReleaseDate = NULL
movies$VideoReleaseDate = NULL
movies$IMDB = NULL
str(movies)
movies = unique(movies)
# Take a look at our data again:
str(movies)
movies
# The ward method cares about the distance between clusters using centroid distance, and also the variance in each of the clusters
clusterMovies = hclust(distances, method = "ward")
# Plot the dendrogram
distances = dist(movies[2:20], method = "euclidean")
clusterMovies = hclust(distances, method = "ward")
plot(clusterMovies)
clusterMovies = hclust(distances, method = "ward.D")
plot(clusterMovies)
plot(clusterMovies)
#  label each of the data points according to what cluster it belongs to using the cutree function
tapply(movies$Action, clusterGroups, mean)
clusterGroups = cutree(clusterMovies, k = 10)
tapply(movies$Action, clusterGroups, mean)
tapply(movies$Romance, clusterGroups, mean)
